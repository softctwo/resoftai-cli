#!/usr/bin/env python3
"""
ä½¿ç”¨DeepSeekè¿›è¡Œå®é™…è½¯ä»¶å¼€å‘ä»»åŠ¡çš„æ¼”ç¤º
"""
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent / "src"))

from resoftai.config import Settings
from resoftai.llm.factory import LLMFactory


async def demo_requirement_analysis():
    """æ¼”ç¤ºï¼šéœ€æ±‚åˆ†æ"""
    print("\n" + "=" * 70)
    print("æ¼”ç¤º1: ä½¿ç”¨DeepSeekè¿›è¡Œéœ€æ±‚åˆ†æ")
    print("=" * 70)

    settings = Settings()
    llm = LLMFactory.create(settings.get_llm_config())

    requirement = """
    æˆ‘éœ€è¦å¼€å‘ä¸€ä¸ªåœ¨çº¿å›¾ä¹¦ç®¡ç†ç³»ç»Ÿï¼ŒåŠŸèƒ½åŒ…æ‹¬ï¼š
    1. ç”¨æˆ·æ³¨å†Œå’Œç™»å½•
    2. å›¾ä¹¦çš„å¢åˆ æ”¹æŸ¥
    3. å›¾ä¹¦å€Ÿé˜…å’Œå½’è¿˜
    4. æŸ¥çœ‹å€Ÿé˜…å†å²
    5. ç®¡ç†å‘˜åå°ç®¡ç†
    """

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æå¸ˆã€‚
è¯·åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œæä¾›è¯¦ç»†çš„åŠŸèƒ½æ¨¡å—åˆ’åˆ†å’ŒæŠ€æœ¯å»ºè®®ã€‚
è¾“å‡ºæ ¼å¼è¦æ¸…æ™°ï¼ŒåŒ…æ‹¬ï¼šåŠŸèƒ½æ¨¡å—ã€æŠ€æœ¯æ ˆå»ºè®®ã€æ•°æ®åº“è®¾è®¡è¦ç‚¹ã€‚"""

    response = await llm.generate(
        prompt=f"è¯·åˆ†æä»¥ä¸‹éœ€æ±‚å¹¶ç»™å‡ºä¸“ä¸šå»ºè®®ï¼š\n{requirement}",
        system_prompt=system_prompt
    )

    print(f"\n{response.content}")
    print(f"\n[Tokenä½¿ç”¨: {response.total_tokens}]")


async def demo_architecture_design():
    """æ¼”ç¤ºï¼šç³»ç»Ÿæ¶æ„è®¾è®¡"""
    print("\n" + "=" * 70)
    print("æ¼”ç¤º2: ä½¿ç”¨DeepSeekè¿›è¡Œç³»ç»Ÿæ¶æ„è®¾è®¡")
    print("=" * 70)

    settings = Settings()
    llm = LLMFactory.create(settings.get_llm_config())

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç³»ç»Ÿæ¶æ„å¸ˆã€‚
è¯·è®¾è®¡ä¸€ä¸ªå¯æ‰©å±•ã€é«˜æ€§èƒ½çš„å¾®æœåŠ¡æ¶æ„ã€‚
è€ƒè™‘ï¼šæœåŠ¡æ‹†åˆ†ã€æ•°æ®åº“è®¾è®¡ã€ç¼“å­˜ç­–ç•¥ã€æ¶ˆæ¯é˜Ÿåˆ—ç­‰ã€‚"""

    prompt = """
    ä¸ºä¸€ä¸ªç”µå•†ç³»ç»Ÿè®¾è®¡å¾®æœåŠ¡æ¶æ„ï¼Œéœ€è¦åŒ…å«ï¼š
    - ç”¨æˆ·æœåŠ¡
    - å•†å“æœåŠ¡
    - è®¢å•æœåŠ¡
    - æ”¯ä»˜æœåŠ¡
    - åº“å­˜æœåŠ¡

    è¯·æä¾›è¯¦ç»†çš„æ¶æ„å›¾è¯´æ˜å’ŒæŠ€æœ¯é€‰å‹ã€‚
    """

    print("\næ­£åœ¨è®¾è®¡æ¶æ„...")
    print("-" * 70)

    full_response = ""
    async for chunk in llm.generate_stream(
        prompt=prompt,
        system_prompt=system_prompt
    ):
        print(chunk, end="", flush=True)
        full_response += chunk

    print(f"\n\n[å®Œæ•´å›å¤é•¿åº¦: {len(full_response)}å­—ç¬¦]")


async def demo_code_generation():
    """æ¼”ç¤ºï¼šä»£ç ç”Ÿæˆ"""
    print("\n" + "=" * 70)
    print("æ¼”ç¤º3: ä½¿ç”¨DeepSeekç”Ÿæˆç”Ÿäº§çº§ä»£ç ")
    print("=" * 70)

    settings = Settings()
    llm = LLMFactory.create(settings.get_llm_config())

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythoné«˜çº§å·¥ç¨‹å¸ˆã€‚
è¯·ç¼–å†™ç”Ÿäº§çº§ä»£ç ï¼ŒåŒ…å«ï¼š
- å®Œæ•´çš„ç±»å‹æ³¨è§£
- è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²
- å¼‚å¸¸å¤„ç†
- å•å…ƒæµ‹è¯•
- æ€§èƒ½ä¼˜åŒ–"""

    prompt = """
    è¯·å®ç°ä¸€ä¸ªRESTful APIçš„ç”¨æˆ·è®¤è¯ç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š
    1. ç”¨æˆ·æ³¨å†Œï¼ˆé‚®ç®±éªŒè¯ï¼‰
    2. ç”¨æˆ·ç™»å½•ï¼ˆJWT Tokenï¼‰
    3. Tokenåˆ·æ–°
    4. å¯†ç é‡ç½®

    ä½¿ç”¨FastAPIæ¡†æ¶ï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ•°æ®éªŒè¯ã€‚
    """

    response = await llm.generate(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=4000
    )

    print(f"\n{response.content}")
    print(f"\n[Tokenä½¿ç”¨: {response.total_tokens}]")


async def demo_code_review():
    """æ¼”ç¤ºï¼šä»£ç å®¡æŸ¥"""
    print("\n" + "=" * 70)
    print("æ¼”ç¤º4: ä½¿ç”¨DeepSeekè¿›è¡Œä»£ç å®¡æŸ¥")
    print("=" * 70)

    settings = Settings()
    llm = LLMFactory.create(settings.get_llm_config())

    code_to_review = '''
def get_user(user_id):
    db = connect_db()
    user = db.execute("SELECT * FROM users WHERE id = " + str(user_id))
    return user
'''

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚
è¯·ä»ä»¥ä¸‹æ–¹é¢å®¡æŸ¥ä»£ç ï¼š
1. å®‰å…¨æ€§é—®é¢˜ï¼ˆSQLæ³¨å…¥ã€XSSç­‰ï¼‰
2. æ€§èƒ½é—®é¢˜
3. ä»£ç è§„èŒƒ
4. æœ€ä½³å®è·µ
5. æ”¹è¿›å»ºè®®"""

    response = await llm.generate(
        prompt=f"è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç å¹¶æä¾›æ”¹è¿›å»ºè®®ï¼š\n\n```python\n{code_to_review}\n```",
        system_prompt=system_prompt
    )

    print(f"\n{response.content}")
    print(f"\n[Tokenä½¿ç”¨: {response.total_tokens}]")


async def demo_bug_fixing():
    """æ¼”ç¤ºï¼šBugä¿®å¤"""
    print("\n" + "=" * 70)
    print("æ¼”ç¤º5: ä½¿ç”¨DeepSeekè¿›è¡ŒBugåˆ†æå’Œä¿®å¤")
    print("=" * 70)

    settings = Settings()
    llm = LLMFactory.create(settings.get_llm_config())

    buggy_code = '''
def calculate_average(numbers):
    total = 0
    for num in numbers:
        total += num
    return total / len(numbers)

# é—®é¢˜ï¼šå½“ä¼ å…¥ç©ºåˆ—è¡¨æ—¶ä¼šå‡ºç° ZeroDivisionError
'''

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è°ƒè¯•ä¸“å®¶ã€‚
è¯·åˆ†æä»£ç bugï¼Œæä¾›ï¼š
1. Bugçš„æ ¹æœ¬åŸå› 
2. ä¿®å¤åçš„å®Œæ•´ä»£ç 
3. æµ‹è¯•ç”¨ä¾‹
4. é¢„é˜²ç±»ä¼¼é—®é¢˜çš„å»ºè®®"""

    response = await llm.generate(
        prompt=f"ä»¥ä¸‹ä»£ç æœ‰bugï¼Œè¯·åˆ†æå¹¶ä¿®å¤ï¼š\n\n```python\n{buggy_code}\n```",
        system_prompt=system_prompt
    )

    print(f"\n{response.content}")
    print(f"\n[Tokenä½¿ç”¨: {response.total_tokens}]")


async def demo_test_generation():
    """æ¼”ç¤ºï¼šæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ"""
    print("\n" + "=" * 70)
    print("æ¼”ç¤º6: ä½¿ç”¨DeepSeekç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")
    print("=" * 70)

    settings = Settings()
    llm = LLMFactory.create(settings.get_llm_config())

    code_to_test = '''
class ShoppingCart:
    def __init__(self):
        self.items = []

    def add_item(self, item, quantity=1):
        self.items.append({"item": item, "quantity": quantity})

    def remove_item(self, item_name):
        self.items = [i for i in self.items if i["item"] != item_name]

    def get_total(self, prices):
        total = 0
        for item in self.items:
            price = prices.get(item["item"], 0)
            total += price * item["quantity"]
        return total
'''

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•å·¥ç¨‹å¸ˆä¸“å®¶ã€‚
è¯·ä¸ºä»£ç ç¼–å†™å®Œæ•´çš„pytestæµ‹è¯•å¥—ä»¶ï¼ŒåŒ…æ‹¬ï¼š
1. æ­£å¸¸æµç¨‹æµ‹è¯•
2. è¾¹ç•Œæ¡ä»¶æµ‹è¯•
3. å¼‚å¸¸æƒ…å†µæµ‹è¯•
4. Mockå’ŒFixtureçš„ä½¿ç”¨
5. æµ‹è¯•è¦†ç›–ç‡è¦æ±‚"""

    response = await llm.generate(
        prompt=f"ä¸ºä»¥ä¸‹ä»£ç ç¼–å†™å®Œæ•´çš„pytestæµ‹è¯•ï¼š\n\n```python\n{code_to_test}\n```",
        system_prompt=system_prompt
    )

    print(f"\n{response.content}")
    print(f"\n[Tokenä½¿ç”¨: {response.total_tokens}]")


async def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("\n" + "ğŸš€" * 35)
    print(" DeepSeek AI è½¯ä»¶å¼€å‘å…¨æµç¨‹æ¼”ç¤º")
    print("ğŸš€" * 35)

    demos = [
        ("éœ€æ±‚åˆ†æ", demo_requirement_analysis),
        ("æ¶æ„è®¾è®¡", demo_architecture_design),
        ("ä»£ç ç”Ÿæˆ", demo_code_generation),
        ("ä»£ç å®¡æŸ¥", demo_code_review),
        ("Bugä¿®å¤", demo_bug_fixing),
        ("æµ‹è¯•ç”Ÿæˆ", demo_test_generation),
    ]

    print("\nè¯·é€‰æ‹©è¦è¿è¡Œçš„æ¼”ç¤ºï¼š")
    for i, (name, _) in enumerate(demos, 1):
        print(f"  {i}. {name}")
    print(f"  0. è¿è¡Œæ‰€æœ‰æ¼”ç¤º")

    try:
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-6): ").strip()

        if choice == "0":
            # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
            for name, demo_func in demos:
                await demo_func()
                await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        elif choice in ["1", "2", "3", "4", "5", "6"]:
            # è¿è¡Œé€‰ä¸­çš„æ¼”ç¤º
            idx = int(choice) - 1
            await demos[idx][1]()
        else:
            print("æ— æ•ˆçš„é€‰é¡¹")
            return

    except KeyboardInterrupt:
        print("\n\næ¼”ç¤ºå·²å–æ¶ˆ")
        return

    print("\n" + "=" * 70)
    print("æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 70)
    print("\næ€»ç»“:")
    print("âœ“ DeepSeekåœ¨è½¯ä»¶å¼€å‘å„ä¸ªç¯èŠ‚éƒ½è¡¨ç°å‡ºè‰²")
    print("âœ“ æ”¯æŒéœ€æ±‚åˆ†æã€æ¶æ„è®¾è®¡ã€ä»£ç ç”Ÿæˆã€å®¡æŸ¥ã€æµ‹è¯•ç­‰")
    print("âœ“ å“åº”é€Ÿåº¦å¿«ï¼Œä»£ç è´¨é‡é«˜")
    print("âœ“ æ€§ä»·æ¯”ä¼˜ç§€ï¼Œé€‚åˆå¤§è§„æ¨¡åº”ç”¨")


if __name__ == "__main__":
    asyncio.run(main())
